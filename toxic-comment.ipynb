{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxic-comment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timthedev07/toxic-comment-classification/blob/dev/toxic-comment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries."
      ],
      "metadata": {
        "id": "gvYpkMNMl-ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JzSqdJszgUsZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the corpus `stopwords`."
      ],
      "metadata": {
        "id": "O7kFSklyqK4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_igO7mbqPJ5",
        "outputId": "846450a7-95cb-4c0a-d74e-88cc17419325"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom function for cleaning input strings."
      ],
      "metadata": {
        "id": "IApNDsdrmEsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(text: tf.Tensor):\n",
        "    # to lower case\n",
        "    text = tf.strings.lower(text)\n",
        "    # expand contraction\n",
        "    pairs = [\n",
        "        (\"won't\", \"will not\"),\n",
        "        (\"can't\", \"can not\"),\n",
        "        (\"n't\", \" not\"),\n",
        "        (\"'re\", \" are\"),\n",
        "        (\"'s\", \" is\"),\n",
        "        (\"'d\", \" would\"),\n",
        "        (\"'ll\", \" will\"),\n",
        "        (\"'t\", \" not\"),\n",
        "        (\"'ve\", \" have\"),\n",
        "        (\"'m\", \" am\"),\n",
        "    ]\n",
        "    for contracted, replacement in pairs:\n",
        "        text = tf.strings.regex_replace(text, contracted, replacement)\n",
        "    \n",
        "    # clean special symbols\n",
        "    text = tf.strings.regex_replace(text, \"<br />\", \" \")\n",
        "    text = tf.strings.regex_replace(text, r\"\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?\", \" \")\n",
        "    text = tf.strings.regex_replace(text, r'@([A-Za-z0-9_]+)', \" \")\n",
        "    text = tf.strings.regex_replace(text, r\"\\([^)]*\\)\", \" \")\n",
        "    text = tf.strings.regex_replace(text, r\"[^A-Za-z0-9]+\", \" \")\n",
        "\n",
        "    # remove stopwords\n",
        "    for i in stop_words:\n",
        "        text = tf.strings.regex_replace(text, f\"[^A-Za-z0-9_]+{i}[^A-Za-z0-9_]+\", \" \")\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "u0mz9nfFmHoq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the dataset."
      ],
      "metadata": {
        "id": "39iaV1vljsMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./sample_data\n",
        "!rm -rf ./content\n",
        "!rm -rf ./data\n",
        "!mkdir data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp \"/content/gdrive/My Drive/datasets/toxic-comment-classification/train.csv.zip\" data\n",
        "!cp \"/content/gdrive/My Drive/datasets/toxic-comment-classification/test.csv.zip\" data\n",
        "%cd data\n",
        "!unzip -ojq train.csv.zip\n",
        "!unzip -ojq test.csv.zip\n",
        "!rm -rf train.csv.zip\n",
        "!rm -rf test.csv.zip\n",
        "%cd ..\n",
        "data = pd.read_csv(\"data/train.csv\")\n",
        "testData = pd.read_csv(\"data/test.csv\")"
      ],
      "metadata": {
        "id": "0mYKdxqGjtlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f183e6e-4183-4a2b-ad02-79d26b0c0a01"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/data\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    \"clean\": 0,\n",
        "    \"toxic\": 1,\n",
        "    \"severe_toxic\": 2,\n",
        "    \"obscene\": 3,\n",
        "    \"threat\": 4,\n",
        "    \"insult\": 5,\n",
        "    \"identity_hate\": 6,\n",
        "}\n",
        "\n",
        "# adding the new row \"label\" to indicate the label's corresponding number\n",
        "for label, labelNum in labels.items():\n",
        "    if label == \"clean\":\n",
        "        continue\n",
        "    data.loc[data[label] == 1, \"label\"] = labelNum\n",
        "\n",
        "# set to 0(clean) on rows where no value is set for \"label\"\n",
        "data.loc[data[\"label\"].isna(), \"label\"] = 0\n",
        "\n",
        "# change dtype\n",
        "data[\"label\"] = data[\"label\"].astype(np.int8)\n",
        "\n",
        "# one hot encoding for y data\n",
        "y = pd.get_dummies(data[\"label\"])\n",
        "\n",
        "# all comments as x\n",
        "x = data[\"comment_text\"]\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(x, y, shuffle = True, random_state = 42, test_size = 0.2)"
      ],
      "metadata": {
        "id": "o8C3QLjinJOL"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model"
      ],
      "metadata": {
        "id": "9i5OyQSvVaX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 15000\n",
        "SEQUENCE_LENGTH = 120\n",
        "EPOCHS = 10\n",
        "\n",
        "def getTrainedModel(_trainX, _trainY, _testX, _testY):\n",
        "    vectorize_layer = TextVectorization(\n",
        "        standardize=custom_standardization,\n",
        "        max_tokens=VOCAB_SIZE,\n",
        "        output_mode='int',\n",
        "        output_sequence_length=SEQUENCE_LENGTH)\n",
        "\n",
        "    vectorize_layer.adapt(np.concatenate([_trainX, _testX]))\n",
        "\n",
        "    embedding_dim = 32\n",
        "\n",
        "    model = Sequential([\n",
        "        vectorize_layer,\n",
        "        Embedding(VOCAB_SIZE, embedding_dim, name=\"embedding\"),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(len(labels), activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    callback = EarlyStopping(patience=1)\n",
        "\n",
        "    model.fit(\n",
        "        _trainX,\n",
        "        _trainY,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=32,\n",
        "        validation_data=(_testX, _testY),\n",
        "        callbacks = [callback]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = getTrainedModel(trainX, trainY, testX, testY)"
      ],
      "metadata": {
        "id": "TnPChCm5VZw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2547fd1-8b2c-4cfd-8927-de96ebf715b7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3990/3990 [==============================] - 120s 30ms/step - loss: 0.1138 - accuracy: 0.9004 - val_loss: 0.0740 - val_accuracy: 0.9145\n",
            "Epoch 2/10\n",
            "3990/3990 [==============================] - 114s 29ms/step - loss: 0.0703 - accuracy: 0.9196 - val_loss: 0.0649 - val_accuracy: 0.9234\n",
            "Epoch 3/10\n",
            "3990/3990 [==============================] - 121s 30ms/step - loss: 0.0607 - accuracy: 0.9252 - val_loss: 0.0629 - val_accuracy: 0.9257\n",
            "Epoch 4/10\n",
            "3990/3990 [==============================] - 128s 32ms/step - loss: 0.0561 - accuracy: 0.9279 - val_loss: 0.0632 - val_accuracy: 0.9275\n",
            "Epoch 5/10\n",
            "3990/3990 [==============================] - 126s 31ms/step - loss: 0.0534 - accuracy: 0.9286 - val_loss: 0.0640 - val_accuracy: 0.9280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "model.save(\"model\")\n",
        "\n",
        "# compress model\n",
        "!zip -r model.zip model\n",
        "\n",
        "# download model\n",
        "from google.colab import files\n",
        "files.download(\"model.zip\")"
      ],
      "metadata": {
        "id": "s4Jva-OkRZJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on a few samples(Note: the texts shown below may contain inappropriate usage of the English language)"
      ],
      "metadata": {
        "id": "fl13ZH8cT7FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def determineLabel(probabilities):\n",
        "    labelNum = np.where(probabilities == np.amax(probabilities))[0]\n",
        "\n",
        "    for key, val in labels.items():\n",
        "        if val == labelNum:\n",
        "            return key\n",
        "\n",
        "evalX = testData[\"comment_text\"][:10]\n",
        "res = model(evalX).numpy()\n",
        "\n",
        "targetLabels = list(map(determineLabel, res))\n",
        "\n",
        "for i in range(len(labels)):\n",
        "    print(f\"Text:\\n  {custom_standardization(evalX[i])}\\nCategory:\\n  {targetLabels[i]}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcSHcntPU33T",
        "outputId": "99d88493-9c4f-4d1f-d8cc-fcc210d33d09"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            "  b'yo bitch ja rule succesful ever whats hating sad mofuckas bitch slap ur pethedic white faces get kiss ass guys sicken ja rule pride da music man dont diss shit nothin wrong bein like tupac brother fuckin white boys get things right next time '\n",
            "Category:\n",
            "  insult\n",
            "\n",
            "\n",
            "Text:\n",
            "  b' rfc title fine imo '\n",
            "Category:\n",
            "  clean\n",
            "\n",
            "\n",
            "Text:\n",
            "  b' sources zawe ashton lapland '\n",
            "Category:\n",
            "  clean\n",
            "\n",
            "\n",
            "Text:\n",
            "  b' look back source information updated correct form guess source updated shall update information thank message '\n",
            "Category:\n",
            "  clean\n",
            "\n",
            "\n",
            "Text:\n",
            "  b'i anonymously edit articles '\n",
            "Category:\n",
            "  clean\n",
            "\n",
            "\n",
            "Text:\n",
            "  b'thank understanding think highly would revert without discussion '\n",
            "Category:\n",
            "  clean\n",
            "\n",
            "\n",
            "Text:\n",
            "  b'please add nonsense wikipedia edits considered vandalism quickly undone would like experiment please use sandbox instead thank '\n",
            "Category:\n",
            "  clean\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}